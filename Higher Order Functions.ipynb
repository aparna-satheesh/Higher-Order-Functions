{"cells":[{"cell_type":"markdown","source":["### Higher Order Functions\nIn PySpark, higher order functions refer to functions that take other functions as arguments, or return functions as values. These functions can be used to simplify data processing tasks and enable more complex data transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91c0897c-085f-4971-8db4-7a3df0258645","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###map()\n\n- PySpark map (map()) is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD.\n\n**Note** : DataFrame doesnâ€™t have map() transformation to use with DataFrame hence you need to convert DataFrame to RDD first."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82aaa5b3-d39f-4b2a-bd26-2d077986b9ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#importing DataTypes and SQL fucntions \nfrom pyspark.sql.types import StructType,StructField,IntegerType,StringType,FloatType,BooleanType,DoubleType,ArrayType\nfrom pyspark.sql.functions import *\nfrom functools import reduce"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50c390f6-ab4e-4942-b0e4-e3752d456fa9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create a sample dataframe\ndf1 = spark.createDataFrame([(1, \"apArna S\"), (2, \"aiswarya satheesh\"), (3, \"athulya Dev\")], [\"id\", \"value\"])\n\n# convert the dataframe to an RDD\nrdd1 = df1.rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94b9d310-0da5-4342-b9ee-eaaf2a078716","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# define a function to apply to each row in the RDD\ndef proper_case_row(row):\n    id = row[0]\n    value = row[1]\n    new_value = value.title()\n    return (id, new_value)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9f15143-6c5d-4102-b8ad-71650f2a6818","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# use the map() function to apply the transform_row() function to each row in the RDD\nnew_rdd = rdd1.map(proper_case_row)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01a0f635-1f1b-4259-80ec-bdd0aeffcb90","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# convert the transformed RDD back to a dataframe\nnew_df = spark.createDataFrame(new_rdd, [\"id\", \"new_value\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b363709-8fd9-40e4-a0e8-36eb3ea42e5d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# join the original dataframe with the transformed dataframe on the \"id\" column\nresult_df = df1.join(new_df, on=\"id\")\n\n# show the result dataframe\nresult_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2399202a-815d-4c42-91d0-6cea08a91c04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+-----------------+-----------------+\n| id|            value|        new_value|\n+---+-----------------+-----------------+\n|  1|         apArna S|         Aparna S|\n|  2|aiswarya satheesh|Aiswarya Satheesh|\n|  3|      athulya Dev|      Athulya Dev|\n+---+-----------------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----------------+-----------------+\n| id|            value|        new_value|\n+---+-----------------+-----------------+\n|  1|         apArna S|         Aparna S|\n|  2|aiswarya satheesh|Aiswarya Satheesh|\n|  3|      athulya Dev|      Athulya Dev|\n+---+-----------------+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###flatMap()\n\n- flatMap() is a transformation operation in PySpark that is used to flatten an RDD of lists or tuples. It maps each input element to zero or more output elements and flattens the results into a single RDD.\n- flatMap() can be used for a variety of tasks, such as splitting a text file into words, expanding lists of values, and more."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21d0b641-0aa6-4ed5-b996-2a7ea01b87e0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# create a sample RDD with nested lists\nrdd = sc.parallelize([['#analytics', '#deeplearning', '#artificialintelligence', '#python', '#ai'], \n ['#analytics', '#datascience', '#deeplearning', '#visualization', '#dataanalysis'],\n ['#datavisualization', '#machinelearning', '#dataanalysis', '#cloudcomputing', '#neuralnetworks'],\n ['#python', '#datascience', '#dataengineering', '#nlp', '#algorithms'],\n ['#statistics', '#machinelearning', '#deeplearning', '#ai', '#bigdata']])\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff7f94fc-a2c4-4f8f-bb61-b9fb8410f197","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# use flatMap to convert the nested list to a flat list of hashtags\nflat_rdd = rdd.flatMap(lambda x: x)\n\n# use map to create a key-value pair of (hashtag, 1) for each hashtag\nhashtag_counts = flat_rdd.map(lambda x: (x, 1))\n\n# use reduceByKey to count the number of occurrences of each hashtag\nhashtag_occurrences = hashtag_counts.reduceByKey(lambda x, y: x + y)\n\n# convert the RDD to a DataFrame with column names\ndf = hashtag_occurrences.toDF([\"hashtag\", \"count\"])\n\n# show the DataFrame\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c0f9e5f3-dd57-48c7-8644-a2824e4379fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+--------------------+-----+\n|             hashtag|count|\n+--------------------+-----+\n|#artificialintell...|    1|\n|       #dataanalysis|    2|\n|            #bigdata|    1|\n|    #dataengineering|    1|\n|             #python|    2|\n|        #datascience|    2|\n|     #cloudcomputing|    1|\n|      #visualization|    1|\n|     #neuralnetworks|    1|\n|         #statistics|    1|\n|       #deeplearning|    3|\n|                #nlp|    1|\n|          #analytics|    2|\n|                 #ai|    2|\n|  #datavisualization|    1|\n|    #machinelearning|    2|\n|         #algorithms|    1|\n+--------------------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------------------+-----+\n|             hashtag|count|\n+--------------------+-----+\n|#artificialintell...|    1|\n|       #dataanalysis|    2|\n|            #bigdata|    1|\n|    #dataengineering|    1|\n|             #python|    2|\n|        #datascience|    2|\n|     #cloudcomputing|    1|\n|      #visualization|    1|\n|     #neuralnetworks|    1|\n|         #statistics|    1|\n|       #deeplearning|    3|\n|                #nlp|    1|\n|          #analytics|    2|\n|                 #ai|    2|\n|  #datavisualization|    1|\n|    #machinelearning|    2|\n|         #algorithms|    1|\n+--------------------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###reduce()\n- reduce() is an action operation in PySpark that aggregates the elements of an RDD using a specified function. It takes a function that accepts two arguments and returns a single value, and applies that function to the elements of the RDD in a cumulative way, reducing the entire RDD down to a single value.\n- The key advantage of reduce() is that it allows you to perform complex aggregations on large datasets in a parallel and distributed manner, making it an essential tool for big data processing."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff012648-3664-41f4-a122-a5661ad27055","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create an RDD containing network packet data\npacket_data = sc.parallelize([\n    {\"src_ip\": \"192.168.1.1\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 1000, \"bytes_received\": 500},\n    {\"src_ip\": \"192.168.1.2\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 1500, \"bytes_received\": 1000},\n    {\"src_ip\": \"192.168.1.3\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 2000, \"bytes_received\": 750},\n    {\"src_ip\": \"192.168.1.1\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 500, \"bytes_received\": 200},\n    {\"src_ip\": \"192.168.1.2\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 1000, \"bytes_received\": 500},\n    {\"src_ip\": \"192.168.1.3\", \"dest_ip\": \"8.8.8.8\", \"bytes_sent\": 500, \"bytes_received\": 250},\n])\n\n# Define a function to aggregate the bytes sent and received by each IP address\ndef aggregate_bytes_by_ip(x, y):\n    return {\"src_ip\": x[\"src_ip\"], \n            \"bytes_sent\": x[\"bytes_sent\"] + y[\"bytes_sent\"], \n            \"bytes_received\": x[\"bytes_received\"] + y[\"bytes_received\"]}\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0aff4c3d-aebe-45f1-8531-a22060d699d3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ip_byte_counts_rdd = packet_data.map(lambda x: (x[\"src_ip\"], x)).reduceByKey(aggregate_bytes_by_ip)\n#.map(x[src],x) basically converts the rdd into a tuple, such that the ipaddr is the key and the value has all the attr \n#('192.168.1.1', {'src_ip': '192.168.1.1',   'dest_ip': '8.8.8.8', 'bytes_sent': 1000, 'bytes_received': 500})\nip_byte_counts_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba5046f1-2046-4cf7-ab63-678e634641eb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[27]: [('192.168.1.3',\n  {'src_ip': '192.168.1.3', 'bytes_sent': 2500, 'bytes_received': 1000}),\n ('192.168.1.2',\n  {'src_ip': '192.168.1.2', 'bytes_sent': 2500, 'bytes_received': 1500}),\n ('192.168.1.1',\n  {'src_ip': '192.168.1.1', 'bytes_sent': 1500, 'bytes_received': 700})]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[27]: [('192.168.1.3',\n  {'src_ip': '192.168.1.3', 'bytes_sent': 2500, 'bytes_received': 1000}),\n ('192.168.1.2',\n  {'src_ip': '192.168.1.2', 'bytes_sent': 2500, 'bytes_received': 1500}),\n ('192.168.1.1',\n  {'src_ip': '192.168.1.1', 'bytes_sent': 1500, 'bytes_received': 700})]"]}}],"execution_count":0},{"cell_type":"code","source":["# Convert the result to a DataFrame\nip_byte_counts_df = ip_byte_counts_rdd.map(lambda x: (x[0], x[1][\"bytes_sent\"], x[1][\"bytes_received\"])) \\\n                                      .toDF([\"src_ip\", \"bytes_sent\", \"bytes_received\"])\n# Show the result as a DataFrame\nip_byte_counts_df.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00afadf5-cf0c-4844-a5e6-f26af1a283b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-----------+----------+--------------+\n|     src_ip|bytes_sent|bytes_received|\n+-----------+----------+--------------+\n|192.168.1.3|      2500|          1000|\n|192.168.1.2|      2500|          1500|\n|192.168.1.1|      1500|           700|\n+-----------+----------+--------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-----------+----------+--------------+\n|     src_ip|bytes_sent|bytes_received|\n+-----------+----------+--------------+\n|192.168.1.3|      2500|          1000|\n|192.168.1.2|      2500|          1500|\n|192.168.1.1|      1500|           700|\n+-----------+----------+--------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Create a dataframe of employee names and ages\ndf_names_ages = spark.createDataFrame([\n    (\"Alice\", 25),\n    (\"Bob\", 30),\n    (\"Charlie\", 35),\n    (\"David\", 40)\n], [\"name\", \"age\"])\n\n# Create a dataframe of employee addresses\ndf_addresses = spark.createDataFrame([\n    (\"Alice\", \"123 Main St\"),\n    (\"Bob\", \"456 Oak Ave\"),\n    (\"Charlie\", \"789 Elm St\"),\n    (\"David\", \"1011 Maple Ave\")\n], [\"name\", \"address\"])\n\n# Create a dataframe of employee salaries\ndf_salaries = spark.createDataFrame([\n    (\"Alice\", 50000),\n    (\"Bob\", 60000),\n    (\"Charlie\", 70000),\n    (\"David\", 80000)\n], [\"name\", \"salary\"])\n\n# Create a dataframe of employee departments\ndf_departments = spark.createDataFrame([\n    (\"Alice\", \"Sales\"),\n    (\"Bob\", \"Marketing\"),\n    (\"Charlie\", \"Engineering\"),\n    (\"David\", \"Finance\")\n], [\"name\", \"department\"])\n\n# Show the dataframes\ndf_names_ages.show()\ndf_addresses.show()\ndf_salaries.show()\ndf_departments.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2d6c293-43da-46fa-a2c1-f650caeaada3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n|  David| 40|\n+-------+---+\n\n+-------+--------------+\n|   name|       address|\n+-------+--------------+\n|  Alice|   123 Main St|\n|    Bob|   456 Oak Ave|\n|Charlie|    789 Elm St|\n|  David|1011 Maple Ave|\n+-------+--------------+\n\n+-------+------+\n|   name|salary|\n+-------+------+\n|  Alice| 50000|\n|    Bob| 60000|\n|Charlie| 70000|\n|  David| 80000|\n+-------+------+\n\n+-------+-----------+\n|   name| department|\n+-------+-----------+\n|  Alice|      Sales|\n|    Bob|  Marketing|\n|Charlie|Engineering|\n|  David|    Finance|\n+-------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n|  David| 40|\n+-------+---+\n\n+-------+--------------+\n|   name|       address|\n+-------+--------------+\n|  Alice|   123 Main St|\n|    Bob|   456 Oak Ave|\n|Charlie|    789 Elm St|\n|  David|1011 Maple Ave|\n+-------+--------------+\n\n+-------+------+\n|   name|salary|\n+-------+------+\n|  Alice| 50000|\n|    Bob| 60000|\n|Charlie| 70000|\n|  David| 80000|\n+-------+------+\n\n+-------+-----------+\n|   name| department|\n+-------+-----------+\n|  Alice|      Sales|\n|    Bob|  Marketing|\n|Charlie|Engineering|\n|  David|    Finance|\n+-------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# define a function to join two data frames\ndef join_two(df1, df2,):\n    return df1.join(df2, \"name\")\n\n\n# join all four data frames using reduce\ndfs = [df_names_ages,df_addresses,df_salaries,df_departments]\njoined_df = reduce(join_two, dfs)\n\n# show the result\njoined_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1e6ab45-c628-4220-95d1-9557e26c9b9d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------+---+--------------+------+-----------+\n|   name|age|       address|salary| department|\n+-------+---+--------------+------+-----------+\n|  Alice| 25|   123 Main St| 50000|      Sales|\n|    Bob| 30|   456 Oak Ave| 60000|  Marketing|\n|Charlie| 35|    789 Elm St| 70000|Engineering|\n|  David| 40|1011 Maple Ave| 80000|    Finance|\n+-------+---+--------------+------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+---+--------------+------+-----------+\n|   name|age|       address|salary| department|\n+-------+---+--------------+------+-----------+\n|  Alice| 25|   123 Main St| 50000|      Sales|\n|    Bob| 30|   456 Oak Ave| 60000|  Marketing|\n|Charlie| 35|    789 Elm St| 70000|Engineering|\n|  David| 40|1011 Maple Ave| 80000|    Finance|\n+-------+---+--------------+------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_1=spark.read.format('csv').option(\"header\",\"true\").load(\"dbfs:/FileStore/Jan_1_15.csv\")\ndf_2=spark.read.format('csv').option(\"header\",\"true\").load(\"dbfs:/FileStore/Jan_16_31.csv\")\ndf_3=spark.read.format('csv').option(\"header\",\"true\").load(\"dbfs:/FileStore/Feb_1_15.csv\")\ndf_4=spark.read.format('csv').option(\"header\",\"true\").load(\"dbfs:/FileStore/Feb_16_28.csv\")\ndf_1.show(1)\ndf_2.show(1)\ndf_3.show(1)\ndf_4.show(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b85c8293-9c46-4535-928a-0b4cd3957eda","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+-------------+---------+----------+\n| id|       ip_add|  country|      date|\n+---+-------------+---------+----------+\n|  1|190.89.215.76|Indonesia|10/14/2022|\n+---+-------------+---------+----------+\nonly showing top 1 row\n\n+---+--------------+-------+--------+\n| id|        ip_add|country|    date|\n+---+--------------+-------+--------+\n|  1|46.136.237.248|  China|5/7/2022|\n+---+--------------+-------+--------+\nonly showing top 1 row\n\n+---+-------------+-------+---------+\n| id|       ip_add|country|     date|\n+---+-------------+-------+---------+\n|  1|29.23.123.248|  China|1/13/2023|\n+---+-------------+-------+---------+\nonly showing top 1 row\n\n+---+--------------+-------+---------+\n| id|        ip_add|country|     date|\n+---+--------------+-------+---------+\n|  1|41.172.148.183|  China|1/13/2023|\n+---+--------------+-------+---------+\nonly showing top 1 row\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------------+---------+----------+\n| id|       ip_add|  country|      date|\n+---+-------------+---------+----------+\n|  1|190.89.215.76|Indonesia|10/14/2022|\n+---+-------------+---------+----------+\nonly showing top 1 row\n\n+---+--------------+-------+--------+\n| id|        ip_add|country|    date|\n+---+--------------+-------+--------+\n|  1|46.136.237.248|  China|5/7/2022|\n+---+--------------+-------+--------+\nonly showing top 1 row\n\n+---+-------------+-------+---------+\n| id|       ip_add|country|     date|\n+---+-------------+-------+---------+\n|  1|29.23.123.248|  China|1/13/2023|\n+---+-------------+-------+---------+\nonly showing top 1 row\n\n+---+--------------+-------+---------+\n| id|        ip_add|country|     date|\n+---+--------------+-------+---------+\n|  1|41.172.148.183|  China|1/13/2023|\n+---+--------------+-------+---------+\nonly showing top 1 row\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["dfs = [df_1, df_2, df_3, df_4]\nresult = reduce(lambda left, right: left.union(right), dfs)\n\nresult.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd82e5b4-0bef-4cf8-b135-1fcc49fdd55d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+---------------+--------------+----------+\n| id|         ip_add|       country|      date|\n+---+---------------+--------------+----------+\n|  1|  190.89.215.76|     Indonesia|10/14/2022|\n|  2|   28.52.25.228|     Indonesia|  3/6/2022|\n|  3| 222.148.60.207|       Ukraine| 7/21/2022|\n|  4|  98.240.246.84|     Indonesia|  8/3/2022|\n|  5|165.212.126.145|        Russia|10/31/2022|\n|  6|177.204.154.141|          Peru|  9/6/2022|\n|  7|  80.251.247.56|     Indonesia|  9/4/2022|\n|  8| 107.155.20.254|     Indonesia| 11/6/2022|\n|  9| 201.189.63.244|         China|  7/9/2022|\n| 10|177.238.208.225|   Afghanistan| 2/12/2022|\n| 11|202.198.239.143|         China| 10/7/2022|\n| 12| 24.117.152.203|     Argentina| 1/11/2023|\n| 13|   122.96.53.14|      Honduras| 3/23/2022|\n| 14| 168.195.251.82|        Belize|  7/4/2022|\n| 15|  59.161.50.131|         China| 9/21/2022|\n| 16|   231.220.6.41|Czech Republic| 3/15/2022|\n| 17| 240.140.109.54|       Ukraine| 6/16/2022|\n| 18|   79.218.69.45|      Bulgaria|  1/8/2022|\n| 19|  194.24.200.74|        Sweden| 5/23/2022|\n| 20|  108.223.92.41|         China| 9/22/2022|\n+---+---------------+--------------+----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------------+--------------+----------+\n| id|         ip_add|       country|      date|\n+---+---------------+--------------+----------+\n|  1|  190.89.215.76|     Indonesia|10/14/2022|\n|  2|   28.52.25.228|     Indonesia|  3/6/2022|\n|  3| 222.148.60.207|       Ukraine| 7/21/2022|\n|  4|  98.240.246.84|     Indonesia|  8/3/2022|\n|  5|165.212.126.145|        Russia|10/31/2022|\n|  6|177.204.154.141|          Peru|  9/6/2022|\n|  7|  80.251.247.56|     Indonesia|  9/4/2022|\n|  8| 107.155.20.254|     Indonesia| 11/6/2022|\n|  9| 201.189.63.244|         China|  7/9/2022|\n| 10|177.238.208.225|   Afghanistan| 2/12/2022|\n| 11|202.198.239.143|         China| 10/7/2022|\n| 12| 24.117.152.203|     Argentina| 1/11/2023|\n| 13|   122.96.53.14|      Honduras| 3/23/2022|\n| 14| 168.195.251.82|        Belize|  7/4/2022|\n| 15|  59.161.50.131|         China| 9/21/2022|\n| 16|   231.220.6.41|Czech Republic| 3/15/2022|\n| 17| 240.140.109.54|       Ukraine| 6/16/2022|\n| 18|   79.218.69.45|      Bulgaria|  1/8/2022|\n| 19|  194.24.200.74|        Sweden| 5/23/2022|\n| 20|  108.223.92.41|         China| 9/22/2022|\n+---+---------------+--------------+----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###filter()\n\n- The filter() function in PySpark is a transformation that creates a new RDD by selecting elements from an existing RDD that satisfy a given condition."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c498de88-0221-4f4f-a198-7ee77249c1b0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["appliance_rdd = sc.parallelize([(\"refrigerator\", \"In Warranty\"), (\"dishwasher\", \"Out of Warranty\"),\n                                (\"microwave\", \"In Warranty\"), (\"oven\", \"Out of Warranty\"),\n                                (\"washer\", \"Out of Warranty\"), (\"dryer\", \"In Warranty\"),\n                                (\"range\", \"In Warranty\"), (\"cooktop\", \"Out of Warranty\")])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e58fee79-b581-4afb-bcf3-c2310a44c4d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["out_of_warranty_rdd = appliance_rdd.filter(lambda x: x[1] == \"Out of Warranty\")\nout_of_warranty_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74c89c59-0aa1-40f7-8a8c-016d716aaf24","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[17]: [('dishwasher', 'Out of Warranty'),\n ('oven', 'Out of Warranty'),\n ('washer', 'Out of Warranty'),\n ('cooktop', 'Out of Warranty')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: [('dishwasher', 'Out of Warranty'),\n ('oven', 'Out of Warranty'),\n ('washer', 'Out of Warranty'),\n ('cooktop', 'Out of Warranty')]"]}}],"execution_count":0},{"cell_type":"code","source":["appliance_df= appliance_rdd.toDF([\"Appliance\", \"Warranty\"])\n# Filter the DataFrame to only include \"Out of Warranty\" rows\nout_of_warranty_df = appliance_df.filter(col(\"Warranty\") == \"Out of Warranty\")\nout_of_warranty_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7feff5ec-6f92-4f96-8920-7b501aa625a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+----------+---------------+\n| Appliance|       Warranty|\n+----------+---------------+\n|dishwasher|Out of Warranty|\n|      oven|Out of Warranty|\n|    washer|Out of Warranty|\n|   cooktop|Out of Warranty|\n+----------+---------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+---------------+\n| Appliance|       Warranty|\n+----------+---------------+\n|dishwasher|Out of Warranty|\n|      oven|Out of Warranty|\n|    washer|Out of Warranty|\n|   cooktop|Out of Warranty|\n+----------+---------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###transform() \n- In PySpark, the transform() function is used to apply a user-defined function (UDF) to each element of an RDD or DataFrame, and returns a new RDD or DataFrame with the transformed values.\n- The ***transform()*** function is similar to the map() function, but allows you to apply more complex transformations that require additional dependencies or external libraries. The function takes a UDF as an argument, which can be defined using either a Python lambda function or a standalone function."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f12190a2-a8f2-4e14-b7f3-0d117362691a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\n# Create a sample DataFrame with a sentence column\ndata = [(\"Subject: enron methanol ; meter : 988291 Subject\",),\n        (\"Subject: ehronline web address change\",),\n        (\"Subject: photoshop , windows , office . cheap . main trending\",)]\ndf = spark.createDataFrame(data, [\"sentence\"])\n\n# Define a function to count the number of words in a sentence\ndef word_count(sentence):\n    return [len(sentence.split())]\n\ndf = df.transform(lambda df: df.select(\"*\", size(split(\"sentence\", \" \")).alias(\"word_count\")))\n\n# Display the resulting DataFrame\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df397862-458f-4d69-8cbb-b5d9577509b8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+-------------------------------------------------------------+----------+\n|sentence                                                     |word_count|\n+-------------------------------------------------------------+----------+\n|Subject: enron methanol ; meter : 988291 Subject             |8         |\n|Subject: ehronline web address change                        |5         |\n|Subject: photoshop , windows , office . cheap . main trending|11        |\n+-------------------------------------------------------------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------------------------------------------------------+----------+\n|sentence                                                     |word_count|\n+-------------------------------------------------------------+----------+\n|Subject: enron methanol ; meter : 988291 Subject             |8         |\n|Subject: ehronline web address change                        |5         |\n|Subject: photoshop , windows , office . cheap . main trending|11        |\n+-------------------------------------------------------------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###exists()\n\n- In PySpark, ***exists()*** is a DataFrame function that checks if a specified column or a set of columns exists in the DataFrame. The function returns a boolean value, True if the specified column(s) exist in the DataFrame, and False otherwise."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49ebddc5-a1cc-4fad-96d1-abef6f9243ae","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["schema = StructType([\n    StructField(\"sensor_id\", IntegerType(), True),\n    StructField(\"sensor_data\", ArrayType(FloatType()), True)\n])\n\n# create a list of tuples with sensor id and sensor data\ndata = [(1, [1.0,1.4,4.5,-2.0,0.0,1.0]), (2, [3.8,2.0,1.9,2.0,0.0,-1.0]), (3, [0.4,1.9,0.0,8.0,3.0,-1.0])]\n\n# create the DataFrame\ndf = spark.createDataFrame(data, schema)\n\n# print the DataFrame\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34e55e29-7ed8-4d6a-b650-dcb15c8679d4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---------+--------------------+\n|sensor_id|         sensor_data|\n+---------+--------------------+\n|        1|[1.0, 1.4, 4.5, -...|\n|        2|[3.8, 2.0, 1.9, 2...|\n|        3|[0.4, 1.9, 0.0, 8...|\n+---------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+\n|sensor_id|         sensor_data|\n+---------+--------------------+\n|        1|[1.0, 1.4, 4.5, -...|\n|        2|[3.8, 2.0, 1.9, 2...|\n|        3|[0.4, 1.9, 0.0, 8...|\n+---------+--------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.select(\"*\",(exists(\"sensor_data\", lambda x: x >5).alias(\"any_negative\"))).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9afbfebf-3065-4351-b453-9613c68d0d16","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---------+-------------------------------+------------+\n|sensor_id|sensor_data                    |any_negative|\n+---------+-------------------------------+------------+\n|1        |[1.0, 1.4, 4.5, -2.0, 0.0, 1.0]|false       |\n|2        |[3.8, 2.0, 1.9, 2.0, 0.0, -1.0]|false       |\n|3        |[0.4, 1.9, 0.0, 8.0, 3.0, -1.0]|true        |\n+---------+-------------------------------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+-------------------------------+------------+\n|sensor_id|sensor_data                    |any_negative|\n+---------+-------------------------------+------------+\n|1        |[1.0, 1.4, 4.5, -2.0, 0.0, 1.0]|false       |\n|2        |[3.8, 2.0, 1.9, 2.0, 0.0, -1.0]|false       |\n|3        |[0.4, 1.9, 0.0, 8.0, 3.0, -1.0]|true        |\n+---------+-------------------------------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["###aggregate()\n- The aggregate() function in PySpark is a transformation operation that applies an aggregation function to the elements of an RDD and returns a result."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f927e93-f3d8-4545-9d8d-f68992de455a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = df.withColumn('avg_sensor_value', aggregate('sensor_data', lit(0.0), lambda acc1, acc2: acc1 + acc2) / size('sensor_data'))\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e982d08-00a7-4f51-8355-484a8fa355be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---------+--------------------+------------------+\n|sensor_id|         sensor_data|  avg_sensor_value|\n+---------+--------------------+------------------+\n|        1|[1.0, 1.4, 4.5, -...|0.9833333293596903|\n|        2|[3.8, 2.0, 1.9, 2...| 1.449999988079071|\n|        3|[0.4, 1.9, 0.0, 8...|2.0499999970197678|\n+---------+--------------------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+--------------------+------------------+\n|sensor_id|         sensor_data|  avg_sensor_value|\n+---------+--------------------+------------------+\n|        1|[1.0, 1.4, 4.5, -...|0.9833333293596903|\n|        2|[3.8, 2.0, 1.9, 2...| 1.449999988079071|\n|        3|[0.4, 1.9, 0.0, 8...|2.0499999970197678|\n+---------+--------------------+------------------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Higher Order Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2775955753775957,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1940973048590456}},"nbformat":4,"nbformat_minor":0}
